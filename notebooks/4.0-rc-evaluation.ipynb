{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f1b78b",
   "metadata": {},
   "source": [
    "# Evaluation — Métricas & Submissão API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, joblib, numpy as np, pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DATA_RAW = \"../data/raw\"\n",
    "DATA_PROCESSED = \"../data/processed\"\n",
    "model = joblib.load(os.path.join(DATA_PROCESSED,\"model_baseline.joblib\"))\n",
    "X_valid = pd.read_csv(\"../data/interim/X_valid.csv\")\n",
    "y_valid = pd.read_csv(\"../data/interim/y_valid.csv\")\n",
    "target_cols = [\"falha_maquina\",\"FDF\",\"FDC\",\"FP\",\"FTE\",\"FA\"]\n",
    "y_pred = (model.predict_proba(X_valid) >= 0.5).astype(int)\n",
    "print(classification_report(y_valid[target_cols], y_pred, target_names=target_cols, zero_division=0))\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(DATA_RAW, \"bootcamp_test.csv\"))\n",
    "proba = model.predict_proba(test_df)\n",
    "import numpy as np\n",
    "proba_mat = np.column_stack(proba) if isinstance(proba, list) else proba\n",
    "submission = pd.DataFrame(proba_mat, columns=target_cols)\n",
    "os.makedirs(DATA_PROCESSED, exist_ok=True)\n",
    "submission_path = os.path.join(DATA_PROCESSED, \"bootcamp_submission.csv\")\n",
    "submission.to_csv(submission_path, index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Escolher o melhor entre RF e XGB pela métrica de CV\n",
    "\n",
    "if xgb_search.best_score_ > rf_search.best_score_ :\n",
    "    best_name = \"xgboost\"\n",
    "    best_pipe = xgb_search.best_estimator_\n",
    "    print(f\"\\n>> Melhor modelo (CV): {best_name}\")\n",
    "else:\n",
    "    best_name = \"random_forest\"\n",
    "    best_pipe = rf_search.best_estimator_\n",
    "\n",
    "print(f\"\\n>> Melhor modelo (CV): {best_name}\")\n",
    "\n",
    "# === Avaliar no holdout\n",
    "y_pred  = best_pipe.predict(X_test)\n",
    "y_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nHoldout:\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1-score :\", f1_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n",
    "\n",
    "# Criar o explainer\n",
    "explainer = ClassifierExplainer(lr_pipe_baseline, X_test, y_test)\n",
    "\n",
    "# Configurar e executar o dashboard com parâmetros específicos\n",
    "dashboard = ExplainerDashboard(explainer, \n",
    "                             title=\"Predictive Maintenance Classifier Explainer\",\n",
    "                             whatif=False,  # desativa a aba what-if\n",
    "                             shap_interaction=False,  # desativa interações SHAP\n",
    "                             decision_trees=False)  # desativa visualização de árvores\n",
    "\n",
    "# Executar o dashboard com configurações específicas para Windows\n",
    "dashboard.run(host='127.0.0.1',  # usar localhost\n",
    "             port=8050,  \n",
    "             use_waitress=True)  # usar servidor waitress que é mais estável no Windows\n",
    "\n",
    "# gere um arquivo html para o dashboard\n",
    "dashboard.save_html(\"../reports/classifier_explainer/LogReg_baseline.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2f1c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
