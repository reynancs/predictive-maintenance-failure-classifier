{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f077d56",
   "metadata": {},
   "source": [
    "\n",
    "# 3.0 — Data Preprocessing (Bootcamp CDIA)\n",
    "\n",
    "**Objetivo:** preparar os dados limpos (`bootcamp_train_processed.csv`) para modelagem em manutenção preditiva (multi‑rótulo).  \n",
    "**Metodologia:** CRISP‑DM — etapa de *Data Preparation* focada em: encoding, escala, balanceamento, engenharia de atributos e pipelines reprodutíveis.\n",
    "\n",
    "> Arquivo esperado em: `/mnt/data/bootcamp_train_processed.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac812b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Imports ===\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Optional: imbalanced-learn (se disponível)\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except Exception:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "print(\"Libraries loaded. imbalanced-learn available:\", IMBLEARN_AVAILABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ca40b",
   "metadata": {},
   "source": [
    "## 1) Carregar dados limpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796511ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Caminho do dataset limpo\n",
    "DATA_PATH = \"/mnt/data/bootcamp_train_processed.csv\"\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Arquivo não encontrado: {DATA_PATH}\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(3))\n",
    "display(df.describe(include='all').T.head(12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d7cd2",
   "metadata": {},
   "source": [
    "## 2) Definir variáveis‑alvo (multi‑rótulo) e features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ca577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alvos esperados conforme enunciado do desafio\n",
    "expected_targets = ['falha_maquina', 'FDF', 'FDC', 'FP', 'FTE', 'FA']\n",
    "target_cols = [c for c in expected_targets if c in df.columns]\n",
    "\n",
    "# Colunas categóricas/numéricas\n",
    "categorical_features = [c for c in df.columns if df[c].dtype == 'object']\n",
    "# Remove alvos e IDs da lista\n",
    "id_like = [c for c in df.columns if 'id' in c.lower()]\n",
    "numeric_features = [c for c in df.select_dtypes(include=[np.number]).columns if c not in target_cols + id_like]\n",
    "\n",
    "# Remover 'tipo' das numéricas se estiver como object por engano\n",
    "if 'tipo' in df.columns and df['tipo'].dtype == 'object' and 'tipo' not in categorical_features:\n",
    "    categorical_features.append('tipo')\n",
    "if 'tipo' in numeric_features and df['tipo'].dtype != np.number:\n",
    "    numeric_features.remove('tipo')\n",
    "\n",
    "print(\"Targets:\", target_cols)\n",
    "print(\"Categóricas:\", categorical_features)\n",
    "print(\"Numéricas:\", numeric_features[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67fb1e",
   "metadata": {},
   "source": [
    "## 3) Funções utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ff042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_distribution(y_df):\n",
    "    dist = {}\n",
    "    for c in y_df.columns:\n",
    "        vc = y_df[c].value_counts(dropna=False).to_dict()\n",
    "        pos = vc.get(1, 0)\n",
    "        neg = vc.get(0, 0)\n",
    "        dist[c] = {\"positivos\": int(pos), \"negativos\": int(neg), \"pct_pos\": round(100*pos/(pos+neg+1e-9),2)}\n",
    "    return pd.DataFrame(dist).T.sort_index()\n",
    "\n",
    "def choose_scaler(name='standard'):\n",
    "    name = (name or '').lower()\n",
    "    if name == 'minmax':\n",
    "        return MinMaxScaler()\n",
    "    if name == 'robust':\n",
    "        return RobustScaler()\n",
    "    return StandardScaler()  # default\n",
    "\n",
    "def add_feature_engineering(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "    # Exemplos de features derivadas (robustas a valores inválidos)\n",
    "    if set(['temperatura_processo','temperatura_ar']).issubset(df_out.columns):\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df_out['ratio_temp_proc_ar'] = df_out['temperatura_processo'] / df_out['temperatura_ar']\n",
    "    if set(['torque','velocidade_rotacional']).issubset(df_out.columns):\n",
    "        df_out['potencia_proxy'] = df_out['torque'] * df_out['velocidade_rotacional']\n",
    "    # Tratar inf e NaN gerados\n",
    "    df_out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_out.fillna(df_out.median(numeric_only=True), inplace=True)\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64536ab9",
   "metadata": {},
   "source": [
    "## 4) (Opcional) Aplicar engenharia de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa905b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "APPLY_FEATURE_ENGINEERING = True\n",
    "X_full = df.drop(columns=[c for c in target_cols])\n",
    "if APPLY_FEATURE_ENGINEERING:\n",
    "    X_full = add_feature_engineering(X_full)\n",
    "\n",
    "y_full = df[target_cols].astype(int) if target_cols else pd.DataFrame()\n",
    "\n",
    "print(\"X_full shape:\", X_full.shape, \"| y_full shape:\", y_full.shape)\n",
    "display(label_distribution(y_full))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835fda1a",
   "metadata": {},
   "source": [
    "## 5) Split treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c841cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Para multilabel, não há estratificação nativa simples -> usaremos split aleatório fixo\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Train:\", X_train.shape, y_train.shape, \"| Test:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4a456",
   "metadata": {},
   "source": [
    "## 6) Preprocessamento (One‑Hot + Scaler) com Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCALER_NAME = 'standard'  # opções: 'standard', 'minmax', 'robust'\n",
    "scaler = choose_scaler(SCALER_NAME)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler, [c for c in X_train.columns if c in numeric_features or np.issubdtype(X_train[c].dtype, np.number)]),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), [c for c in X_train.columns if c in categorical_features])\n",
    "    ],\n",
    "    \n",
    ")\n",
    "\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be744cee",
   "metadata": {},
   "source": [
    "## 7) Abordagens para desbalanceamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ba0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_SMOTE = False and IMBLEARN_AVAILABLE  # altere para True se quiser testar SMOTE (se disponível)\n",
    "\n",
    "if USE_SMOTE and IMBLEARN_AVAILABLE:\n",
    "    print(\"SMOTE será aplicado dentro de um fluxo manual após o fit_transform do preprocessor.\")\n",
    "else:\n",
    "    print(\"Usando class_weight='balanced' no modelo (recomendado como baseline).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e992a2",
   "metadata": {},
   "source": [
    "## 8) Pipeline de modelagem (baseline: Logistic Regression One‑Vs‑Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelo base\n",
    "base_estimator = LogisticRegression(max_iter=200, class_weight='balanced', solver='lbfgs', n_jobs=None)\n",
    "clf = OneVsRestClassifier(base_estimator, n_jobs=None)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('prep', preprocessor),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986bd2a",
   "metadata": {},
   "source": [
    "## 9) Treinamento e avaliação rápida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f21e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predições\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.decision_function(X_test) if hasattr(pipeline.named_steps['clf'], \"decision_function\") else pipeline.predict_proba(X_test)\n",
    "\n",
    "# Relatório por classe\n",
    "for i, col in enumerate(y_test.columns):\n",
    "    print(f\"\\n=== Classe: {col} ===\")\n",
    "    print(classification_report(y_test[col], y_pred[:, i]))\n",
    "    try:\n",
    "        # Para multilabel binário, podemos calcular ROC AUC por classe (se proba disponível)\n",
    "        if isinstance(y_proba, list):  # alguns estimadores retornam lista por classe\n",
    "            proba_i = y_proba[i][:,1] if y_proba[i].ndim==2 else y_proba[i]\n",
    "        else:\n",
    "            # decision_function pode retornar score contínuo; roc_auc aceita\n",
    "            proba_i = y_proba[:, i]\n",
    "        auc = roc_auc_score(y_test[col], proba_i)\n",
    "        print(\"ROC AUC:\", round(auc, 4))\n",
    "    except Exception as e:\n",
    "        print(\"ROC AUC não disponível:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72a3e5",
   "metadata": {},
   "source": [
    "## 10) (Opcional) Exportar dados pré‑processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXPORT_TRANSFORMED = False\n",
    "\n",
    "if EXPORT_TRANSFORMED:\n",
    "    Xtr = pipeline.named_steps['prep'].fit_transform(X_train)\n",
    "    Xte = pipeline.named_steps['prep'].transform(X_test)\n",
    "    np.save('/mnt/data/X_train_preprocessed.npy', Xtr)\n",
    "    np.save('/mnt/data/X_test_preprocessed.npy', Xte)\n",
    "    y_train.to_csv('/mnt/data/y_train.csv', index=False)\n",
    "    y_test.to_csv('/mnt/data/y_test.csv', index=False)\n",
    "    print(\"Arquivos salvos em /mnt/data/*.npy e /mnt/data/*.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad12f2d",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Próximos passos sugeridos\n",
    "- **Validação**: adicionar `KFold` (ou `RepeatedKFold`) e medir métricas por classe com intervalos.\n",
    "- **Tuning**: Grid/Random Search para `C`, `penalty`, `solver` e tipo de `Scaler`.\n",
    "- **Modelos**: testar `LinearSVC`, `RandomForest`, `XGBoost`/`LightGBM` (se disponíveis) via `OneVsRest`.\n",
    "- **Thresholding**: calibrar limiar por classe (otimizar F1/Recall).\n",
    "- **Balanceamento**: avaliar SMOTE/ADASYN quando disponível (cuidado com vazamento de dados).\n",
    "- **Feature Engineering**: iterar nas features derivadas e checar importâncias/SHAP em modelos de árvore.\n",
    "- **Export**: persistir `pipeline` com `joblib` para uso posterior (API de avaliação do bootcamp).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5a52b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c885777",
   "metadata": {},
   "source": [
    "# Data Preparation — Manutenção Preditiva (CNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATA_RAW = \"../data/raw\"\n",
    "DATA_INTERIM = \"../data/interim\"\n",
    "os.makedirs(DATA_INTERIM, exist_ok=True)\n",
    "df = pd.read_csv(os.path.join(DATA_RAW, \"bootcamp_train.csv\"))\n",
    "df[\"torque_por_rpm\"] = df[\"torque\"] / df[\"velocidade_rotacional\"].replace(0, np.nan)\n",
    "target_cols = [\"falha_maquina\",\"FDF\",\"FDC\",\"FP\",\"FTE\",\"FA\"]\n",
    "X = df.drop(columns=target_cols)\n",
    "Y = df[target_cols]\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "preprocess = ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y[\"falha_maquina\"])\n",
    "X_train.to_csv(os.path.join(DATA_INTERIM,\"X_train.csv\"),index=False)\n",
    "X_valid.to_csv(os.path.join(DATA_INTERIM,\"X_valid.csv\"),index=False)\n",
    "y_train.to_csv(os.path.join(DATA_INTERIM,\"y_train.csv\"),index=False)\n",
    "y_valid.to_csv(os.path.join(DATA_INTERIM,\"y_valid.csv\"),index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
